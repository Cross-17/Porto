{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Crossbell\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named 'numba'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-490628e2b8a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnumba\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjit\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'numba'"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "\"\"\"\n",
    "This simple scripts demonstrates the use of xgboost eval results to get the best round\n",
    "for the current fold and accross folds. \n",
    "It also shows an upsampling method that limits cross-validation overfitting.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import gc\n",
    "from numba import jit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time \n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "@jit\n",
    "def eval_gini(y_true, y_prob):\n",
    "    \"\"\"\n",
    "    Original author CPMP : https://www.kaggle.com/cpmpml\n",
    "    In kernel : https://www.kaggle.com/cpmpml/extremely-fast-gini-computation\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_true = y_true[np.argsort(y_prob)]\n",
    "    ntrue = 0\n",
    "    gini = 0\n",
    "    delta = 0\n",
    "    n = len(y_true)\n",
    "    for i in range(n-1, -1, -1):\n",
    "        y_i = y_true[i]\n",
    "        ntrue += y_i\n",
    "        gini += y_i * delta\n",
    "        delta += 1 - y_i\n",
    "    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n",
    "    return gini\n",
    "\n",
    "def gini_xgb(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    gini_score = eval_gini(labels, preds)\n",
    "    return [('gini', gini_score)]\n",
    "\n",
    "\n",
    "def add_noise(series, noise_level):\n",
    "    return series * (1 + noise_level * np.random.randn(len(series)))\n",
    "\n",
    "\n",
    "def target_encode(trn_series=None,\n",
    "                  tst_series=None,\n",
    "                  target=None,\n",
    "                  min_samples_leaf=1,\n",
    "                  smoothing=1,\n",
    "                  noise_level=0):\n",
    "    \"\"\"\n",
    "    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n",
    "    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n",
    "    trn_series : training categorical feature as a pd.Series\n",
    "    tst_series : test categorical feature as a pd.Series\n",
    "    target : target data as a pd.Series\n",
    "    min_samples_leaf (int) : minimum samples to take category average into account\n",
    "    smoothing (int) : smoothing effect to balance categorical average vs prior\n",
    "    \"\"\"\n",
    "    assert len(trn_series) == len(target)\n",
    "    assert trn_series.name == tst_series.name\n",
    "    temp = pd.concat([trn_series, target], axis=1)\n",
    "    # Compute target mean\n",
    "    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n",
    "    # Compute smoothing\n",
    "    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n",
    "    # Apply average function to all target data\n",
    "    prior = target.mean()\n",
    "    # The bigger the count the less full_avg is taken into account\n",
    "    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n",
    "    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n",
    "    # Apply averages to trn and tst series\n",
    "    ft_trn_series = pd.merge(\n",
    "        trn_series.to_frame(trn_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=trn_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_trn_series.index = trn_series.index\n",
    "    ft_tst_series = pd.merge(\n",
    "        tst_series.to_frame(tst_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=tst_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_tst_series.index = tst_series.index\n",
    "    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)\n",
    "\n",
    "gc.enable()\n",
    "\n",
    "base_path = '/input'\n",
    "trn_df = pd.read_csv(base_path + \"train.csv\", index_col=0)\n",
    "sub_df = pd.read_csv(base_path + \"test.csv\", index_col=0)\n",
    "\n",
    "target = trn_df[\"target\"]\n",
    "del trn_df[\"target\"]\n",
    "\n",
    "train_features = [\n",
    "    \"ps_car_13\",  #            : 1571.65 / shadow  609.23\n",
    "\"ps_reg_03\",  #            : 1408.42 / shadow  511.15\n",
    "\"ps_ind_05_cat\",  #        : 1387.87 / shadow   84.72\n",
    "\"ps_ind_03\",  #            : 1219.47 / shadow  230.55\n",
    "\"ps_ind_15\",  #            :  922.18 / shadow  242.00\n",
    "\"ps_reg_02\",  #            :  920.65 / shadow  267.50\n",
    "\"ps_car_14\",  #            :  798.48 / shadow  549.58\n",
    "\"ps_car_12\",  #            :  731.93 / shadow  293.62\n",
    "\"ps_car_01_cat\",  #        :  698.07 / shadow  178.72\n",
    "\"ps_car_07_cat\",  #        :  694.53 / shadow   36.35\n",
    "\"ps_car_03_cat\",  #        :  611.73 / shadow   50.67\n",
    "\"ps_reg_01\",  #            :  598.60 / shadow  178.57\n",
    "\"ps_car_15\",  #            :  593.35 / shadow  226.43\n",
    "\"ps_ind_01\",  #            :  547.32 / shadow  154.58\n",
    "\"ps_ind_161718_bin\",  #        :  475.37 / shadow   34.17\n",
    "\"ps_ind_0609_bin\",  #        :  435.28 / shadow   28.92\n",
    "\"ps_car_06_cat\",  #        :  398.02 / shadow  212.43\n",
    "\"ps_car_04_cat\",  #        :  376.87 / shadow   76.98\n",
    "\"ps_car_09_cat\",  #        :  214.12 / shadow   81.38\n",
    "\"ps_car_02_cat\",  #        :  203.03 / shadow   26.67\n",
    "\"ps_ind_02_cat\",  #        :  189.47 / shadow   65.68\n",
    "\"ps_car_11\",  #            :  173.28 / shadow   76.45\n",
    "\"ps_car_05_cat\",  #        :  172.75 / shadow   62.92\n",
    "\"ps_calc_09\",  #           :  169.13 / shadow  129.72\n",
    "\"ps_calc_05\",  #           :  148.83 / shadow  120.68\n",
    "\"ps_car_08_cat\",  #        :  120.87 / shadow   28.82\n",
    "\"ps_ind_04_cat\",  #        :  107.27 / shadow   37.43\n",
    "\"ps_ind_12_bin\",  #        :   39.67 / shadow   15.52\n",
    "\"ps_ind_14\",  #            :   37.37 / shadow   16.65\n",
    "]\n",
    "# add combinations\n",
    "combs = [\n",
    "    ('ps_reg_01', 'ps_car_02_cat'),  \n",
    "    ('ps_reg_01', 'ps_car_04_cat'),\n",
    "]\n",
    "start = time.time()\n",
    "for n_c, (f1, f2) in enumerate(combs):\n",
    "    name1 = f1 + \"_plus_\" + f2\n",
    "    print('current feature %60s %4d in %5.1f'\n",
    "          % (name1, n_c + 1, (time.time() - start) / 60), end='')\n",
    "    print('\\r' * 75, end='')\n",
    "    trn_df[name1] = trn_df[f1].apply(lambda x: str(x)) + \"_\" + trn_df[f2].apply(lambda x: str(x))\n",
    "    sub_df[name1] = sub_df[f1].apply(lambda x: str(x)) + \"_\" + sub_df[f2].apply(lambda x: str(x))\n",
    "    # Label Encode\n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(list(trn_df[name1].values) + list(sub_df[name1].values))\n",
    "    trn_df[name1] = lbl.transform(list(trn_df[name1].values))\n",
    "    sub_df[name1] = lbl.transform(list(sub_df[name1].values))\n",
    "\n",
    "    train_features.append(name1)\n",
    "    \n",
    "trn_df = trn_df[train_features]\n",
    "sub_df = sub_df[train_features]\n",
    "\n",
    "f_cats = [f for f in trn_df.columns if \"_cat\" in f]\n",
    "\n",
    "for f in f_cats:\n",
    "    trn_df[f + \"_avg\"], sub_df[f + \"_avg\"] = target_encode(trn_series=trn_df[f],\n",
    "                                         tst_series=sub_df[f],\n",
    "                                         target=target,\n",
    "                                         min_samples_leaf=200,\n",
    "                                         smoothing=10,\n",
    "                                         noise_level=0)\n",
    "\n",
    "n_splits = 5\n",
    "n_estimators = 200\n",
    "folds = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=15) \n",
    "imp_df = np.zeros((len(trn_df.columns), n_splits))\n",
    "xgb_evals = np.zeros((n_estimators, n_splits))\n",
    "oof = np.empty(len(trn_df))\n",
    "sub_preds = np.zeros(len(sub_df))\n",
    "sub_preds_train = np.zeros(len(trn_df))\n",
    "increase = True\n",
    "np.random.seed(0)\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(target, target)):\n",
    "    trn_dat, trn_tgt = trn_df.iloc[trn_idx], target.iloc[trn_idx]\n",
    "    val_dat, val_tgt = trn_df.iloc[val_idx], target.iloc[val_idx]\n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "    params = {'n_estimators':n_estimators,\n",
    "                        'max_depth':4,\n",
    "                        'objective':\"binary:logistic\",\n",
    "                        'learning_rate':.1, \n",
    "                        'subsample':.8, \n",
    "                        'colsample_bytree':.8,\n",
    "                        'gamma':1,\n",
    "                        'reg_alpha':0,\n",
    "                        'reg_lambda':1,\n",
    "                        'nthread':2}\n",
    "    # Upsample during cross validation to avoid having the same samples\n",
    "    # in both train and validation sets\n",
    "    # Validation set is not up-sampled to monitor overfitting\n",
    "    if increase:\n",
    "        # Get positive examples\n",
    "        pos = pd.Series(trn_tgt == 1)\n",
    "        # Add positive examples\n",
    "        trn_dat = pd.concat([trn_dat, trn_dat.loc[pos]], axis=0)\n",
    "        trn_tgt = pd.concat([trn_tgt, trn_tgt.loc[pos]], axis=0)\n",
    "        # Shuffle data\n",
    "        idx = np.arange(len(trn_dat))\n",
    "        np.random.shuffle(idx)\n",
    "        trn_dat = trn_dat.iloc[idx]\n",
    "        trn_tgt = trn_tgt.iloc[idx]\n",
    "        \n",
    "    d_train = xgb.DMatrix(trn_dat, trn_tgt) \n",
    "    d_valid = xgb.DMatrix(val_dat, val_tgt) \n",
    "    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "        \n",
    "    clf = xgb.train(params, d_train, 10**6, watchlist, early_stopping_rounds=20, \n",
    "                          feval=gini_xgb, maximize=True, verbose_eval=100)\n",
    "            \n",
    "\n",
    "    sub_preds += clf.predict(xgb.DMatrix(sub_df), ntree_limit=clf.best_ntree_limit) / n_splits\n",
    "    sub_preds_train += clf.predict(xgb.DMatrix(trn_df), ntree_limit=clf.best_ntree_limit) / n_splits\n",
    "\n",
    "    # Display results\n",
    "#     print(\"Fold %2d : %.6f @%4d / best score is %.6f @%4d\"\n",
    "#           % (fold_ + 1,\n",
    "#              eval_gini(val_tgt, oof[val_idx]),\n",
    "#              n_estimators,\n",
    "#              xgb_evals[best_round, fold_],\n",
    "#              best_round))\n",
    "          \n",
    "print(\"Full OOF score : %.6f\" % eval_gini(target, oof))\n",
    "\n",
    "# Compute mean score and std\n",
    "mean_eval = np.mean(xgb_evals, axis=1)\n",
    "std_eval = np.std(xgb_evals, axis=1)\n",
    "best_round = np.argsort(mean_eval)[::-1][0]\n",
    "\n",
    "print(\"Best mean score : %.6f + %.6f @%4d\"\n",
    "      % (mean_eval[best_round], std_eval[best_round], best_round))\n",
    "    \n",
    "importances = sorted([(trn_df.columns[i], imp) for i, imp in enumerate(imp_df.mean(axis=1))],\n",
    "                     key=lambda x: x[1])\n",
    "\n",
    "for f, imp in importances[::-1]:\n",
    "    print(\"%-34s : %10.4f\" % (f, imp))\n",
    "    \n",
    "sub_df[\"target\"] = sub_preds\n",
    "trn_df[\"target\"] = sub_preds_train\n",
    "\n",
    "sub_df[[\"target\"]].to_csv(base_path + \"test_submission.csv\", index=True, float_format=\"%.9f\")\n",
    "trn_df[[\"target\"]].to_csv(base_path + \"train_submission.csv\", index=True, float_format=\"%.9f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
